{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Crawler Module\n",
    "Design and implement a crawler module that can collect paper title, author list, publication time, and abstract from PUBMED for a given keyword (i.e., HIV) within a pre-specified time window (that is, 01/01/2020 – 09/01/2020), and the retrieved data should be saved in the CSV format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install biopython first by opening the anaconda prompt and typing in \"pip install biopython\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Main function #################33\n",
    "def Manue_search():\n",
    "    inputKw = input(\"Please type in a keyword you are interested in: eg. HIV: \")\n",
    "    inputTw = input(\"Please type in a time window for searching in format yyyy/mm/dd - yyyy/mm/dd: eg. 2020/08/29 - 2020/08/30: \")\n",
    "    inputemail = input(\"Please type your email address:\")    \n",
    "    try:\n",
    "        inputTw_min = inputTw.split(\"-\")[0].strip()\n",
    "        inputTw_max = inputTw.split(\"-\")[1].strip()\n",
    "        Papers_name = Extract_Raw_Info(inputKw, inputTw_min, inputTw_max, inputemail)\n",
    "        Get_info_all(Papers_name)\n",
    "    except HTTPError:\n",
    "        print(\"Please retry, connection with NCBI is not stable\")\n",
    "    except:\n",
    "        print(\"Make sure you type in the right format for time window, please rerun the current function.\")\n",
    "\n",
    "\n",
    "############### Get required information of one paper##############################33\n",
    "#Input: all information for a paper\n",
    "#Output: a list of extract infromation: title, author, publish date, and abstract\n",
    "def Get_info_each(ppcon):\n",
    "    #Organize the original file for one pubmed id\n",
    "    fsm1 = list(map(lambda a: a.split(\"- \"), ppcon))\n",
    "    #Get the length of each element of data\n",
    "    Each_len = list(map(len, fsm1))\n",
    "    \n",
    "    #Get the position of first line of each required information in extrated data list\n",
    "    #Information: Ti: paper title, FAU: author list, DP: publication time, and AB: abstract\n",
    "    ##extract first element--name of that line, in data\n",
    "    Names = list(map(lambda a:a.strip(),next(zip(*fsm1))))\n",
    "    DuidFun = lambda a: (i for i,value in enumerate(Names) if value == a)\n",
    "    Ti_id = list(DuidFun(\"TI\"))\n",
    "    FAU_id = list(DuidFun(\"FAU\"))\n",
    "    DP_id = list(DuidFun(\"DP\"))\n",
    "    AB_id = list(DuidFun(\"AB\"))\n",
    "\n",
    "    #Get the information of FAU and DEP\n",
    "    if len(FAU_id)!=0:\n",
    "        FAU_final = \"; \".join(list(fsm1[x][1] for x in FAU_id))\n",
    "    else:\n",
    "        FAU_final = None\n",
    "\n",
    "    if len(DP_id)!=0:\n",
    "        DP_final = fsm1[DP_id[0]][1]\n",
    "    else:\n",
    "        DP_final = None\n",
    "\n",
    "    #Get the information of Ti and AB\n",
    "    if len(Ti_id)!=0:\n",
    "        Ti_end = Ti_id[0] + Each_len[Ti_id[0]+1:].index(2)\n",
    "        Ti_full = [fsm1[Ti_id[0]][1]] + list(map(lambda a: a[0].strip(),fsm1[Ti_id[0] + 1:Ti_end + 1]))\n",
    "        Ti_final = \" \".join(Ti_full)\n",
    "    else:\n",
    "        Ti_final = None\n",
    "\n",
    "    if len(AB_id)!=0:\n",
    "        AB_end = AB_id[0] + Each_len[AB_id[0]+1:].index(2)\n",
    "        AB_full = [fsm1[AB_id[0]][1]] + list(map(lambda a: a[0].strip(),fsm1[AB_id[0] + 1:AB_end + 1]))\n",
    "        AB_final = \" \".join(AB_full)\n",
    "    else:\n",
    "        AB_final = None\n",
    "     \n",
    "    #[Title = Ti_final, Author = FAU_final, Publish_date = DEP_final, Abstract = AB_final]\n",
    "    return([Ti_final, FAU_final, DP_final, AB_final])\n",
    "\n",
    "\n",
    "###########Interect with NCBI and get all infromation####################\n",
    "#Note: this step might has have error with \"Error 500: Internal Server Error\", Ignor it, just retry\n",
    "#During working hours, the line might be very crowded, you may get more 500 error T.T. Be patient, you can do it!\n",
    "\n",
    "def Extract_Raw_Info(inputKw, inputTw_min, inputTw_max, inputemail):\n",
    "    #############Search papers if with input query#######################################################\n",
    "    #Tell NCBI who you are\n",
    "    Entrez.email = inputemail\n",
    "    handle_srch = Entrez.esearch(db = \"pubmed\", term = inputKw, usehistory = \"y\", datatype = 'pdat', mindate = inputTw_min, maxdate = inputTw_max)\n",
    "    record_srch = Entrez.read(handle_srch)\n",
    "\n",
    "    count = int(record_srch[\"Count\"])\n",
    "    print(\"Found %i results\" % count)\n",
    "\n",
    "    ##############Using searched id information to get A file with all information needed###################\n",
    "    Papers_name = \"_\".join([\"Papers\", inputKw,inputTw_min.replace(\"/\", \"-\"), \"to\", inputTw_max.replace(\"/\",\"-\"), \".txt\"])\n",
    "    batch_size = 10\n",
    "    out_handle = open(Papers_name, \"w\")\n",
    "    for start in range(0, count, batch_size):\n",
    "        end = min(count, start + batch_size)\n",
    "        print(\"Going to download record %i to %i\" % (start + 1, end))\n",
    "        fetch_handle = Entrez.efetch(\n",
    "            db=\"pubmed\",\n",
    "            rettype=\"medline\",\n",
    "            retmode=\"text\",\n",
    "            retstart=start,\n",
    "            retmax=batch_size,\n",
    "            webenv=record_srch[\"WebEnv\"],\n",
    "            query_key=record_srch[\"QueryKey\"],\n",
    "        )\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        out_handle.write(data)\n",
    "    out_handle.close()\n",
    "    return(Papers_name)\n",
    "\n",
    "\n",
    "\n",
    "#############Read information of saved file#########################\n",
    "#Input: Generated file with all information for papers\n",
    "#Output: A csv file generated with all extracted infromation: title, author, publish date, and abstract\n",
    "def Get_info_all(Papers_name): \n",
    "    All_info_op = open(Papers_name)\n",
    "    All_info = All_info_op.read()\n",
    "    All_info_op.close() \n",
    "\n",
    "    Aifs = All_info.split(\"\\n\")\n",
    "    # Get the position of each record in the text\n",
    "    RcID = [i for i,value in enumerate(list(map(len, Aifs))) if value == 0]\n",
    "    print(\"The number of papers extracted is: %i \" % (len(RcID) - 1))\n",
    "\n",
    "    Info_df = pd.DataFrame(np.zeros(((len(RcID) - 1),4)), columns=['Title', 'Author', 'Publish_date','Abstract'])\n",
    "    for paperi in range(len(RcID) - 1):\n",
    "        #Set a id for interested paper\n",
    "        #paperi = 4\n",
    "        #Paper content\n",
    "        print(\"Paper %i is running\" % paperi)\n",
    "        ppcon = Aifs[RcID[paperi]:RcID[paperi + 1]]\n",
    "        Info_df.loc[paperi,:] = Get_info_each(ppcon)\n",
    "    \n",
    "    Info_name = \"_\".join([\"Info_from\",Papers_name]).replace(\"_.txt\", \".csv\")\n",
    "    Info_df.to_csv(Info_name, index = False)\n",
    "\n",
    "Manue_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Database and Query Module\n",
    "Design and implement a database module that can import the CSV file to SQLite to build a database automatically. Then implement SQL code to query the publications by author’s name (i.e., input an author’s name and find out and return all his/her publications). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the above modules, create an empty sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('part2.db').touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a database connection and cursor to execute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('part2.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute a query that will create an \"HIV_Papers\" table with \"Title\", \"Author\", \"Publish_date\", and \"Abstract\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('''CREATE TABLE HIV_Papers (Title text, Author text, Publish_date date, Abstract text)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the CSV file into a dataframe. Then, write the data from the dataframe to the SQLite table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivpapers = pd.read_csv('Info_from_Papers_HIV_2020-08-29_to_2020-08-30.csv')\n",
    "hivpapers.to_sql('HIV_Papers', conn, if_exists='append', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt user to input author name. If the author name is found, the output returns the author's name, all co-authors, and the title of the publication. If there are multiple results of that name, each result will show on a new line. If the author's name is not found, the output will give an error message and repeat the prompt input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_author = input('Enter an author name, e.g. Nguyen: ') \n",
    "    result = c.execute(f\"SELECT Author, Title FROM HIV_Papers WHERE Author LIKE '%{input_author}%'\")\n",
    "    res = result.fetchall()\n",
    "    if len(res)!=0:\n",
    "        for row in res:\n",
    "            print(row)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Sorry, the author name you entered does not match any authors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Visualization Dashboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's import all the modules we might be requiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ipyfilechooser import FileChooser\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's read the csv file we want to analyze today. Please choose the csv file you would like to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "fc = FileChooser()\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "CSV = pandas.read_csv(fc.selected)\n",
    "\n",
    "# here are the first 3 rows of the file you chose to read\n",
    "CSV.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of publications in each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the string of publications\n",
    "split_dates = CSV['Publish_date'].str.split('\\ ')\n",
    "\n",
    "# save how many publication we have\n",
    "num_publications = len(split_dates)\n",
    "\n",
    "# loop through all publications and extract dates\n",
    "year_list = []\n",
    "for i in range(num_publications):\n",
    "    # get current publication year\n",
    "    year_list.append( split_dates[i][0])\n",
    "\n",
    "\n",
    "# get the vector of unique years\n",
    "unique_years = list(set(year_list))\n",
    "\n",
    "# make a data frame of counts per year\n",
    "publications_per_year = pandas.DataFrame(columns = [\"Year\",\n",
    "                                                    \"Publications\"])\n",
    "for i in range(len(unique_years)):\n",
    "    # get current year\n",
    "    year = unique_years[i]\n",
    "    \n",
    "    # get current counts\n",
    "    cnts = year_list.count(str(year))\n",
    "    \n",
    "    # make df\n",
    "    pubs_this_year = pandas.DataFrame(data = [[year,\n",
    "                                               cnts]], \n",
    "                                    columns = [\"Year\",\n",
    "                                               \"Publications\"])\n",
    "    \n",
    "    # append to out data frame\n",
    "    publications_per_year = publications_per_year.append(pubs_this_year,\n",
    "                                                         ignore_index=True)\n",
    "\n",
    "# show some rows\n",
    "publications_per_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend of the publication numbers over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show publications over time\n",
    "seaborn.histplot(year_list)\n",
    "plt.xlabel(\"Year of Publication\")\n",
    "plt.ylabel(\"Number of Publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and visualize the summary statistics for the publication number per year, including mean, SD, range, median, 1st to 3rd quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "mn = publications_per_year.Publications.mean()\n",
    "med = publications_per_year.Publications.median()\n",
    "sdev = publications_per_year.Publications.std()\n",
    "q25 = publications_per_year.Publications.quantile(0.25)\n",
    "q75 = publications_per_year.Publications.quantile(0.75)\n",
    "\n",
    "# Make a table\n",
    "Statistics_Table = pandas.DataFrame(data = [[mn,\n",
    "                                             med,\n",
    "                                            sdev,\n",
    "                                            q25,\n",
    "                                            q75]],\n",
    "                                    columns = [\"Mean\",\n",
    "                                               \"Median\",\n",
    "                                              'Standard Deviation',\n",
    "                                              'First Quantile',\n",
    "                                              'Third Quantile'])\n",
    "\n",
    "# show table\n",
    "Statistics_Table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
